reproducibility_configs:
    "reproduce": False
    "random_seed": None

dataloader_configs:
    "batch_size": 512
    "num_workers": 32
    "shuffle": True

lr_scheduler_configs:
    "warmup": linear
    "warmup_steps": 10000
    "decay": linear

circuit_configs:
    "mask_method": "continuous_sparsification"
    "mask_hparams":
        "ablation": "none"
        "mask_unit": "weight"
        "mask_bias": False
        "mask_init_value": -0.1
    "freeze_base" : True
    "add_l0" : True
    "l0_lambda" : 1.0e-8

train_configs:
    "wandb_project": "vit_transfer"
    "n_epochs": 40    
    "hf_pretrained": "google/vit-base-patch16-224" # Pretrained to load for original ResNet & Feature Extractor
    "original": "checkpoints/vit_base_16class" # The 16-class adapted model path, model should match w prev line
    "subnet": "checkpoints/vit_ablated_1e-8_b32"
    "output_path": "checkpoints/vit_1e-8_transfer_adam"
    "verbose": True # Will print more intermediate status if True

### Optimizer configs
optim_configs:
    "optim_name": "Adam" # Name of any optimizer class in torch.nn.optim
    "weight_decay": 0.03
    "lr": 1.0e-3